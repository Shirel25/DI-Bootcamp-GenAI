{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff9e6e19-0e04-4064-b692-bdc070403756",
   "metadata": {},
   "source": [
    "# 1. Install Required Libraries\n",
    "Ensure you have the necessary libraries installed, including gensim, spacy, torch, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d3c759-b982-4172-b01d-80509b6641e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in ./.local/lib/python3.10/site-packages (25.1.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.13-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.local/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.local/lib/python3.10/site-packages (from spacy) (2.32.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached blis-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached blis-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/lib/python3/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.3)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Using cached triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Using cached spacy-3.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.5 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.13-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "Using cached preshed-3.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (795 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "Using cached blis-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Using cached smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "Using cached torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, cymem, wrapt, wasabi, typing-inspection, triton, threadpoolctl, sympy, spacy-loggers, spacy-legacy, shellingham, pydantic-core, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, murmurhash, mdurl, marisa-trie, joblib, fsspec, filelock, cloudpathlib, catalogue, annotated-types, srsly, smart-open, scipy, pydantic, preshed, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, markdown-it-py, language-data, blis, scikit-learn, rich, nvidia-cusolver-cu12, langcodes, gensim, confection, typer, torch, thinc, weasel, spacy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55/55\u001b[0m [spacy]m [spacy]m [weasel]es]olver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 filelock-3.18.0 fsspec-2025.7.0 gensim-4.3.3 joblib-1.5.1 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 murmurhash-1.0.13 networkx-3.4.2 numpy-1.26.4 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 rich-14.0.0 scikit-learn-1.7.1 scipy-1.13.1 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 sympy-1.14.0 thinc-8.3.4 threadpoolctl-3.6.0 torch-2.7.1 triton-3.3.1 typer-0.16.0 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kaggle in ./.local/lib/python3.10/site-packages (1.7.4.5)\n",
      "Requirement already satisfied: bleach in ./.local/lib/python3.10/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/lib/python3/dist-packages (from kaggle) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer in ./.local/lib/python3.10/site-packages (from kaggle) (3.4.2)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from kaggle) (3.3)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.10/site-packages (from kaggle) (6.31.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.local/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in ./.local/lib/python3.10/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from kaggle) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/lib/python3/dist-packages (from kaggle) (59.6.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in ./.local/lib/python3.10/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/lib/python3/dist-packages (from kaggle) (1.26.5)\n",
      "Requirement already satisfied: webencodings in ./.local/lib/python3.10/site-packages (from kaggle) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install gensim spacy torch scikit-learn\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58740478-4e73-417f-88fa-da6fc6a2d5e6",
   "metadata": {},
   "source": [
    "# 2. Load and Preprocess the Dataset\n",
    "\n",
    "- Download the stock market dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdecce31-bf36-477e-918d-17e09126f7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset\n",
      "License(s): CC0-1.0\n",
      "stock-market-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "# Kaggle Authentification\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Downloading the stock market dataset\n",
    "!kaggle datasets download -d jacksoncrow/stock-market-dataset\n",
    "\n",
    "# Unzip\n",
    "# !unzip -q stock-market-dataset.zip -d stock_data\n",
    "!unzip -o -q stock-market-dataset.zip -d stock_data # -o force l'ecrasement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067ec9a-237b-4f1b-8a82-363576b53470",
   "metadata": {},
   "source": [
    "- Drop unnecessary columns and create a target column for the next day’s closing price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac5a5d-c070-4e73-a621-58ee35825292",
   "metadata": {},
   "source": [
    "To begin the analysis, we chose to work with a single CSV file from the dataset: AAPL.csv, which contains historical stock prices for Apple Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc44c23c-d05c-44d3-a234-543b32372ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pandas]2m1/2\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.1 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc15bd09-ee69-45f0-bdf6-48478631cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date      Open      High       Low     Close  Adj Close     Volume\n",
      "0  1980-12-12  0.513393  0.515625  0.513393  0.513393   0.406782  117258400\n",
      "1  1980-12-15  0.488839  0.488839  0.486607  0.486607   0.385558   43971200\n",
      "2  1980-12-16  0.453125  0.453125  0.450893  0.450893   0.357260   26432000\n",
      "3  1980-12-17  0.462054  0.464286  0.462054  0.462054   0.366103   21610400\n",
      "4  1980-12-18  0.475446  0.477679  0.475446  0.475446   0.376715   18362400\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9909 entries, 0 to 9908\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       9909 non-null   object \n",
      " 1   Open       9909 non-null   float64\n",
      " 2   High       9909 non-null   float64\n",
      " 3   Low        9909 non-null   float64\n",
      " 4   Close      9909 non-null   float64\n",
      " 5   Adj Close  9909 non-null   float64\n",
      " 6   Volume     9909 non-null   int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 542.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"stock_data/stocks/AAPL.csv\")\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93e2ee13-c059-4ff5-8b37-8962a318729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date     Close    Target\n",
      "0 1980-12-12  0.513393  0.486607\n",
      "1 1980-12-15  0.486607  0.450893\n",
      "2 1980-12-16  0.450893  0.462054\n",
      "3 1980-12-17  0.462054  0.475446\n",
      "4 1980-12-18  0.475446  0.504464\n"
     ]
    }
   ],
   "source": [
    "## Drop unnecessary columns\n",
    "df = df.drop(columns=['Adj Close'])  # often close to 'Close', so not necessary\n",
    "\n",
    "# Convert 'Date' to datetime \n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Class the DataFrame by date \n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "## Create a target column for the next day’s closing price\n",
    "df['Target'] = df['Close'].shift(-1)\n",
    "\n",
    "# Drop last line (NaN) \n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Verification\n",
    "print(df[['Date', 'Close', 'Target']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f460a18-9e01-4f4c-94a0-0620ca041dc9",
   "metadata": {},
   "source": [
    "- Normalize the dataset using MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b82b52c-ae8d-4363-9f7e-04c9615f846e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Open      High       Low     Close    Volume    Target\n",
      "0  0.000970  0.000967  0.000981  0.000969  0.063023  0.486607\n",
      "1  0.000894  0.000886  0.000898  0.000887  0.023516  0.450893\n",
      "2  0.000784  0.000777  0.000787  0.000778  0.014061  0.462054\n",
      "3  0.000812  0.000811  0.000822  0.000812  0.011462  0.475446\n",
      "4  0.000853  0.000852  0.000863  0.000853  0.009711  0.504464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select the columns\n",
    "feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "target_col = 'Target'\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the scaler on the features\n",
    "df_scaled = df.copy()\n",
    "df_scaled[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "# Print a part\n",
    "print(df_scaled[feature_cols + [target_col]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1b3eb-ba13-4848-9d3f-73c66ae3a3a8",
   "metadata": {},
   "source": [
    "# 3. Prepare the Dataset for Training\n",
    "- Split the dataset into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79d34933-5840-4ade-a5b3-320d1f68a1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size : 6935\n",
      "Validation size : 1486\n",
      "Test size : 1487\n"
     ]
    }
   ],
   "source": [
    "# Total size dataset\n",
    "n = len(df_scaled)\n",
    "\n",
    "# Percentage of split\n",
    "train_size = int(n * 0.7)\n",
    "val_size = int(n * 0.15)\n",
    "test_size = n - train_size - val_size  # the rest\n",
    "\n",
    "# Split of the data\n",
    "train_df = df_scaled.iloc[:train_size]\n",
    "val_df = df_scaled.iloc[train_size:train_size + val_size]\n",
    "test_df = df_scaled.iloc[train_size + val_size:]\n",
    "\n",
    "# Verification\n",
    "print(\"Train size :\", len(train_df))\n",
    "print(\"Validation size :\", len(val_df))\n",
    "print(\"Test size :\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d59651-e41b-447a-8a56-617b405dfe9c",
   "metadata": {},
   "source": [
    "- Create a custom PyTorch Dataset class to handle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbac4c7f-eecf-4b16-8824-751c050e02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, feature_cols, target_col, sequence_length=30):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.features = data[feature_cols].values.astype('float32')\n",
    "        self.targets = data[target_col].values.astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        # nombre of possible sequence\n",
    "        return len(self.features) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # X sequence : 30 days from idx\n",
    "        X = self.features[idx:idx + self.sequence_length]\n",
    "\n",
    "        # y = target right after the sequence\n",
    "        y = self.targets[idx + self.sequence_length]\n",
    "\n",
    "        return torch.tensor(X), torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f88ef38-eb6f-4ae7-a96d-2775b0fbae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([30, 5])\n",
      "y: tensor(0.5536)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "target_col = 'Target'\n",
    "\n",
    "train_dataset = StockDataset(train_df, feature_cols, target_col, sequence_length=30)\n",
    "val_dataset = StockDataset(val_df, feature_cols, target_col, sequence_length=30)\n",
    "test_dataset = StockDataset(test_df, feature_cols, target_col, sequence_length=30)\n",
    "\n",
    "# Example\n",
    "x, y = train_dataset[0]\n",
    "print(\"X shape:\", x.shape) \n",
    "print(\"y:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b5edbab-642d-4499-8da5-6d0d3150612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Parameters\n",
    "batch_size = 64\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c12f800e-6fda-4bc9-b770-7cad1ee84a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: torch.Size([64, 30, 5])\n",
      "y batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Tester un batch\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"X batch shape:\", x_batch.shape)  # (batch_size, sequence_length, nb_features)\n",
    "    print(\"y batch shape:\", y_batch.shape)  # (batch_size,)\n",
    "    break  # we display the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db90dc9-430f-437e-afbd-c1f70a7b7ee8",
   "metadata": {},
   "source": [
    "# 4. Define the LSTM Model\n",
    "- Create an LSTM model using PyTorch.\n",
    "- Define the model architecture, including GRU layers, dropout, and a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f76db78-8834-4564-9445-db2f02428f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,\n",
    "                            dropout=dropout)\n",
    "\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, input_size)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initial hidden & cell state (num_layers, batch_size, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # LSTM output\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch, seq_len, hidden)\n",
    "\n",
    "        # On prend uniquement la dernière sortie de la séquence (t = -1)\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_size)\n",
    "\n",
    "        # Prediction\n",
    "        out = self.fc(out)  # (batch_size, 1)\n",
    "        return out.squeeze()  # (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8209179d-d656-45e3-b89f-1de5304123f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(5, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(input_size=5, hidden_size=64, num_layers=2)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44afd3cc-893d-4530-a88f-5118aa4dd406",
   "metadata": {},
   "source": [
    "# 5. Train the Model\n",
    "- Set up the optimizer and loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14cae7c8-c3fa-440c-9bb1-d4810b780cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Loss function: regression → Mean Squared Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: Adam is good default\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d401f-85bd-492f-8db8-ef1078de62b8",
   "metadata": {},
   "source": [
    "- Implement training and validation loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13123926-2434-49d4-9be1-0f32b50f4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3db8903e-0007-4ecb-b48b-91aa16cb32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8810d-eb44-4e01-8531-3f3a716859ff",
   "metadata": {},
   "source": [
    "- Train the model for a specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "977ff618-8311-4162-86e4-63f91923266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 21.9369 | Val Loss: 2875.3448\n",
      "Epoch 2/20 | Train Loss: 19.8791 | Val Loss: 2691.6588\n",
      "Epoch 3/20 | Train Loss: 8.1401 | Val Loss: 2009.0984\n",
      "Epoch 4/20 | Train Loss: 2.6983 | Val Loss: 1725.0664\n",
      "Epoch 5/20 | Train Loss: 1.2422 | Val Loss: 1567.7651\n",
      "Epoch 6/20 | Train Loss: 0.7682 | Val Loss: 1470.4648\n",
      "Epoch 7/20 | Train Loss: 0.5811 | Val Loss: 1401.2042\n",
      "Epoch 8/20 | Train Loss: 0.4781 | Val Loss: 1345.5473\n",
      "Epoch 9/20 | Train Loss: 0.4774 | Val Loss: 1305.1665\n",
      "Epoch 10/20 | Train Loss: 0.3712 | Val Loss: 1271.9547\n",
      "Epoch 11/20 | Train Loss: 0.2934 | Val Loss: 1250.5474\n",
      "Epoch 12/20 | Train Loss: 0.2817 | Val Loss: 1227.9176\n",
      "Epoch 13/20 | Train Loss: 0.2803 | Val Loss: 1214.7775\n",
      "Epoch 14/20 | Train Loss: 0.2962 | Val Loss: 1233.8938\n",
      "Epoch 15/20 | Train Loss: 0.3183 | Val Loss: 1237.3802\n",
      "Epoch 16/20 | Train Loss: 0.1944 | Val Loss: 1189.5087\n",
      "Epoch 17/20 | Train Loss: 0.2122 | Val Loss: 1212.5697\n",
      "Epoch 18/20 | Train Loss: 0.2243 | Val Loss: 1169.4932\n",
      "Epoch 19/20 | Train Loss: 0.2053 | Val Loss: 1160.2932\n",
      "Epoch 20/20 | Train Loss: 0.1838 | Val Loss: 1182.5875\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114da2c9-8f64-4aee-8b90-9edb9f684728",
   "metadata": {},
   "source": [
    "# 6. Evaluate the Model\n",
    "- Calculate the R² score to evaluate the model’s performance on the test set.\n",
    "- Save the scaler object for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29d7caca-2a80-4bb7-8d32-e9235332d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def evaluate_model(model, test_loader, scaler_target=None):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            true_values.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Optional: inverse transform if target was scaled\n",
    "    if scaler_target:\n",
    "        predictions = scaler_target.inverse_transform([[p] for p in predictions])\n",
    "        true_values = scaler_target.inverse_transform([[t] for t in true_values])\n",
    "\n",
    "    # Flatten\n",
    "    predictions = [p[0] if isinstance(p, list) else p for p in predictions]\n",
    "    true_values = [t[0] if isinstance(t, list) else t for t in true_values]\n",
    "\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    print(f\"Test R² Score: {r2:.4f}\")\n",
    "    return predictions, true_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f3662a4-1989-4d65-9c91-df5173ef4745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R² Score: -5.9656\n"
     ]
    }
   ],
   "source": [
    "predictions, true_values = evaluate_model(model, test_loader, scaler_target=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a8d44c8-b952-49cf-90e9-56925c4d0d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_scaler.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# We normalized the features\n",
    "joblib.dump(scaler, \"feature_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0664984c-1f1d-437b-a433-2776598adc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load(\"feature_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb72e46-902c-44c6-8a30-3ca4d4ac09d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
