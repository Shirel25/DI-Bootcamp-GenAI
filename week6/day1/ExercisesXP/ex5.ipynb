{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŒŸ **Exercise 5 (optional)** : Implementing Forward and Backward Propagation in Python\n",
        "\n",
        "You will code a simple neural network that performs forward propagation and backpropagation for a regression problem (predicting exam scores based on study hours)."
      ],
      "metadata": {
        "id": "z9ulqfvV3z-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Run the code and observe how the weights and bias update.**"
      ],
      "metadata": {
        "id": "kXsnU5-j4ANv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEboc9Ga3mk9",
        "outputId": "bd1202ac-1f3a-43cb-f2cb-f16b8a82303d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Prediction: 36.4\n",
            "Loss: 1180.98\n",
            "Updated Weights: [ 2.544 39.18 ]\n",
            "Updated Bias: 10.486\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize input data (features)\n",
        "x = np.array([4, 80])  # 4 hours studied, previous test score: 80\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = np.array([0.6, 0.3])  # Initial weights\n",
        "b = 10  # Initial bias\n",
        "\n",
        "# Forward Propagation\n",
        "def forward_propagation(x, w, b):\n",
        "    z = np.dot(x, w) + b  # Weighted sum\n",
        "    return z  # Linear activation (No ReLU here, it's a regression task)\n",
        "\n",
        "# Compute prediction\n",
        "y_pred = forward_propagation(x, w, b)\n",
        "y_true = 85  # Actual exam score\n",
        "\n",
        "# Compute Loss (Mean Squared Error)\n",
        "loss = 0.5 * (y_true - y_pred) ** 2\n",
        "\n",
        "# Compute Gradients\n",
        "grad_w = -(y_true - y_pred) * x  # Partial derivatives with respect to weights\n",
        "grad_b = -(y_true - y_pred)  # Partial derivative with respect to bias\n",
        "\n",
        "# Update Weights and Bias\n",
        "learning_rate = 0.01\n",
        "w_new = w - learning_rate * grad_w\n",
        "b_new = b - learning_rate * grad_b\n",
        "\n",
        "# Print Results\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Updated Weights:\", w_new)\n",
        "print(\"Updated Bias:\", b_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Explain why updating weights using gradient descent reduces the error.**"
      ],
      "metadata": {
        "id": "4umQmucj5tYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating the weights using gradient descent reduces the error because the gradient tells us the direction in which the error increases.\n",
        "By subtracting the gradient from the current weight (new_weight = old_weight - learning_rate * gradient), we move in the opposite direction, toward a lower error.\n",
        "\n",
        "If the gradient is positive, we decrease the weight; if it's negative, subtracting a negative value increases the weight.\n",
        "In both cases, this helps the model reduce the loss and make better predictions."
      ],
      "metadata": {
        "id": "4C7ZBfktJjle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Modify the initial weights or learning rate and see how it affects learning.**\n"
      ],
      "metadata": {
        "id": "V6VVVh3sKIrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   ***Initial weights***\n",
        "\n"
      ],
      "metadata": {
        "id": "YoOY4EyZMtpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize input data (features)\n",
        "x = np.array([4, 80])  # 4 hours studied, previous test score: 80\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = np.array([10, 30])  # Initial weights\n",
        "b = 10  # Initial bias\n",
        "\n",
        "# Forward Propagation\n",
        "def forward_propagation(x, w, b):\n",
        "    z = np.dot(x, w) + b  # Weighted sum\n",
        "    return z  # Linear activation (No ReLU here, it's a regression task)\n",
        "\n",
        "# Compute prediction\n",
        "y_pred = forward_propagation(x, w, b)\n",
        "y_true = 85  # Actual exam score\n",
        "\n",
        "# Compute Loss (Mean Squared Error)\n",
        "loss = 0.5 * (y_true - y_pred) ** 2\n",
        "\n",
        "# Compute Gradients\n",
        "grad_w = -(y_true - y_pred) * x  # Partial derivatives with respect to weights\n",
        "grad_b = -(y_true - y_pred)  # Partial derivative with respect to bias\n",
        "\n",
        "# Update Weights and Bias\n",
        "learning_rate = 0.01\n",
        "w_new = w - learning_rate * grad_w\n",
        "b_new = b - learning_rate * grad_b\n",
        "\n",
        "# Print Results\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Updated Weights:\", w_new)\n",
        "print(\"Updated Bias:\", b_new)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGZsn0HcMrzn",
        "outputId": "f2d1588f-451a-4e90-c045-b34452fda9a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Prediction: 2450\n",
            "Loss: 2796612.5\n",
            "Updated Weights: [  -84.6 -1862. ]\n",
            "Updated Bias: -13.650000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize input data (features)\n",
        "x = np.array([4, 80])  # 4 hours studied, previous test score: 80\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = np.array([1, 1])  # Initial weights\n",
        "b = 10  # Initial bias\n",
        "\n",
        "# Forward Propagation\n",
        "def forward_propagation(x, w, b):\n",
        "    z = np.dot(x, w) + b  # Weighted sum\n",
        "    return z  # Linear activation (No ReLU here, it's a regression task)\n",
        "\n",
        "# Compute prediction\n",
        "y_pred = forward_propagation(x, w, b)\n",
        "y_true = 85  # Actual exam score\n",
        "\n",
        "# Compute Loss (Mean Squared Error)\n",
        "loss = 0.5 * (y_true - y_pred) ** 2\n",
        "\n",
        "# Compute Gradients\n",
        "grad_w = -(y_true - y_pred) * x  # Partial derivatives with respect to weights\n",
        "grad_b = -(y_true - y_pred)  # Partial derivative with respect to bias\n",
        "\n",
        "# Update Weights and Bias\n",
        "learning_rate = 0.01\n",
        "w_new = w - learning_rate * grad_w\n",
        "b_new = b - learning_rate * grad_b\n",
        "\n",
        "# Print Results\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Updated Weights:\", w_new)\n",
        "print(\"Updated Bias:\", b_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c86MMw-x43RD",
        "outputId": "e37a4ed8-e92e-4906-a1cd-df7ce137351e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Prediction: 94\n",
            "Loss: 40.5\n",
            "Updated Weights: [ 0.64 -6.2 ]\n",
            "Updated Bias: 9.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I ran two experiments with different initial weights.\n",
        "The first one used very large weights ([10, 30]), which led to an extreme prediction and unstable updates.\n",
        "\n",
        "The second used more reasonable weights ([1, 1]), leading to a better prediction and smoother learning.\n",
        "\n",
        "Modifying the initial weights has a strong impact on the learning process.\n",
        "When the weights are too large, the prediction becomes extremely far from the true value, which leads to a very large error and unstable gradient updates.\n",
        "\n",
        "In contrast, using more reasonable initial weights (such as [1, 1]) keeps the prediction closer to the target, produces smaller gradients, and results in more stable and effective learning.\n",
        "This experiment shows how important weight initialization is for guiding gradient descent and reducing the loss efficiently."
      ],
      "metadata": {
        "id": "xqHVJ4buNx2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   ***Learning Rate***\n",
        "\n"
      ],
      "metadata": {
        "id": "sn443w9aM12b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize input data (features)\n",
        "x = np.array([4, 80])  # 4 hours studied, previous test score: 80\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = np.array([0.6, 0.3])  # Initial weights\n",
        "b = 10  # Initial bias\n",
        "\n",
        "# Forward Propagation\n",
        "def forward_propagation(x, w, b):\n",
        "    z = np.dot(x, w) + b  # Weighted sum\n",
        "    return z  # Linear activation (No ReLU here, it's a regression task)\n",
        "\n",
        "# Compute prediction\n",
        "y_pred = forward_propagation(x, w, b)\n",
        "y_true = 85  # Actual exam score\n",
        "\n",
        "# Compute Loss (Mean Squared Error)\n",
        "loss = 0.5 * (y_true - y_pred) ** 2\n",
        "\n",
        "# Compute Gradients\n",
        "grad_w = -(y_true - y_pred) * x  # Partial derivatives with respect to weights\n",
        "grad_b = -(y_true - y_pred)  # Partial derivative with respect to bias\n",
        "\n",
        "# Update Weights and Bias\n",
        "learning_rate = 0.1\n",
        "w_new = w - learning_rate * grad_w\n",
        "b_new = b - learning_rate * grad_b\n",
        "\n",
        "# Print Results\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Updated Weights:\", w_new)\n",
        "print(\"Updated Bias:\", b_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROJjqoxjKbpB",
        "outputId": "9dc79204-7925-4119-db86-b131463ec99d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Prediction: 36.4\n",
            "Loss: 1180.98\n",
            "Updated Weights: [ 20.04 389.1 ]\n",
            "Updated Bias: 14.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize input data (features)\n",
        "x = np.array([4, 80])  # 4 hours studied, previous test score: 80\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = np.array([0.6, 0.3])  # Initial weights\n",
        "b = 10  # Initial bias\n",
        "\n",
        "# Forward Propagation\n",
        "def forward_propagation(x, w, b):\n",
        "    z = np.dot(x, w) + b  # Weighted sum\n",
        "    return z  # Linear activation (No ReLU here, it's a regression task)\n",
        "\n",
        "# Compute prediction\n",
        "y_pred = forward_propagation(x, w, b)\n",
        "y_true = 85  # Actual exam score\n",
        "\n",
        "# Compute Loss (Mean Squared Error)\n",
        "loss = 0.5 * (y_true - y_pred) ** 2\n",
        "\n",
        "# Compute Gradients\n",
        "grad_w = -(y_true - y_pred) * x  # Partial derivatives with respect to weights\n",
        "grad_b = -(y_true - y_pred)  # Partial derivative with respect to bias\n",
        "\n",
        "# Update Weights and Bias\n",
        "learning_rate = 0.001\n",
        "w_new = w - learning_rate * grad_w\n",
        "b_new = b - learning_rate * grad_b\n",
        "\n",
        "# Print Results\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Updated Weights:\", w_new)\n",
        "print(\"Updated Bias:\", b_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f6PQjdVOool",
        "outputId": "c221f1a1-8bca-40a9-c3f3-26ed1906ce7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Prediction: 36.4\n",
            "Loss: 1180.98\n",
            "Updated Weights: [0.7944 4.188 ]\n",
            "Updated Bias: 10.0486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tested the impact of different learning rates on the training step.\n",
        "With a high learning rate (0.1), the weight updates were too large and unstable, resulting in extremely high new weights and the risk of divergence.\n",
        "\n",
        "On the other hand, with a small learning rate (0.001), the updates were very small and stable, but learning would be much slower over time.\n",
        "This comparison shows that choosing a good learning rate is essential for balancing speed and stability in training.\n",
        "\n",
        "A learning rate that is too high can lead to overshooting, while one that is too low can make learning inefficient."
      ],
      "metadata": {
        "id": "VjMlvVUPPAHN"
      }
    }
  ]
}